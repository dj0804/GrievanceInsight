{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dd918f9-368e-4e74-87f7-3e1f58eb98dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/avi19/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nltk/corpus/util.py:84\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/avi19/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Define Global Constants\u001b[39;00m\n\u001b[32m     23\u001b[39m CATEGORIES = [\u001b[33m'\u001b[39m\u001b[33mHostel\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mMess\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAcademics\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAdministration\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m ENGLISH_STOPWORDS = \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSetup Complete.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nltk/corpus/util.py:120\u001b[39m, in \u001b[36mLazyCorpusLoader.__getattr__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLazyCorpusLoader object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nltk/corpus/util.py:86\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     84\u001b[39m             root = nltk.data.find(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.subdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[32m     89\u001b[39m corpus = \u001b[38;5;28mself\u001b[39m.__reader_cls(root, *\u001b[38;5;28mself\u001b[39m.__args, **\u001b[38;5;28mself\u001b[39m.__kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nltk/corpus/util.py:81\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/avi19/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Core Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text Preprocessing (NLTK)\n",
    "import nltk\n",
    "# Ensure NLTK data is available\n",
    "try:\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    print(\"Downloading NLTK resources (punkt, stopwords)...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# NLP Models (Hugging Face Transformers - used in subsequent cells)\n",
    "from transformers import pipeline\n",
    "\n",
    "# Define Global Constants\n",
    "CATEGORIES = ['Hostel', 'Mess', 'Academics', 'Administration']\n",
    "ENGLISH_STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "print(\"Setup Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20807a36-8fef-4e0b-9f8c-8f7901ef7881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies cleaning and normalization steps: removes duplicates, anonymizes, \n",
    "    and normalizes text.\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Data Cleaning...\")\n",
    "    # a. Remove duplicate complaints\n",
    "    df.drop_duplicates(subset=['raw_text'], inplace=True)\n",
    "    \n",
    "    # b. Anonymize (Placeholder: simple lowercase/string conversion)\n",
    "    def anonymize_text(text):\n",
    "        if pd.isna(text): return \"\"\n",
    "        return str(text).lower() \n",
    "        \n",
    "    df['clean_text'] = df['raw_text'].apply(anonymize_text)\n",
    "    \n",
    "    # c. Normalize text (remove extra spaces)\n",
    "    df['clean_text'] = df['clean_text'].str.strip()\n",
    "    return df\n",
    "\n",
    "def load_classification_model():\n",
    "    \"\"\"Placeholder for loading a trained scikit-learn classification model.\"\"\"\n",
    "    # In a real project, the model and vectorizer would be loaded here.\n",
    "    return None \n",
    "\n",
    "def classify_complaints(df: pd.DataFrame, classifier=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Classifies complaints into defined categories. Uses simulation if no model is provided.\n",
    "    \"\"\"\n",
    "    print(\"Step 2: Complaint Classification (Simulated)...\")\n",
    "    if classifier:\n",
    "        # Actual model prediction code goes here\n",
    "        pass\n",
    "    \n",
    "    # Simulating classification \n",
    "    weights = [0.35, 0.35, 0.15, 0.15] # Mess and Hostel often have more complaints\n",
    "    df['category'] = np.random.choice(CATEGORIES, size=len(df), p=weights) \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "36d187ac-85f2-4e0a-8e22-e14cfaa39472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_sentiment_analysis(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyzes the emotional tone of complaints. Uses a simulated Hugging Face pipeline \n",
    "    or random choice for portability.\n",
    "    \"\"\"\n",
    "    print(\"Step 3: Sentiment Analysis (Simulated)...\")\n",
    "    \n",
    "    try:\n",
    "        # Attempt to load a simple sentiment pipeline\n",
    "        sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", device=-1)\n",
    "        \n",
    "        def get_sentiment(text):\n",
    "            if not text: return 'Neutral'\n",
    "            try:\n",
    "                result = sentiment_pipeline(text[:250])[0]\n",
    "                return result['label'].capitalize()\n",
    "            except Exception:\n",
    "                return 'Neutral'\n",
    "\n",
    "        df['sentiment'] = df['clean_text'].apply(get_sentiment)\n",
    "        \n",
    "    except Exception:\n",
    "        # Fallback to simulation \n",
    "        df['sentiment'] = np.random.choice(['Negative', 'Neutral', 'Positive'], size=len(df), p=[0.7, 0.2, 0.1])\n",
    "\n",
    "    # Placeholder for 'Urgency' assessment\n",
    "    df['urgency'] = np.random.choice(['High', 'Medium', 'Low'], size=len(df), p=[0.2, 0.5, 0.3])\n",
    "    return df\n",
    "\n",
    "    \n",
    "def summarize_and_extract_trends(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Identifies top recurring issues via frequency and generates a weekly summary \n",
    "    using a transformer model (simulated).\n",
    "    \"\"\"\n",
    "    print(\"Step 4: Summarization and Trend Extraction...\")\n",
    "    results = {}\n",
    "    \n",
    "    # a. Trend Extraction (Top 3 Recurring Issues)\n",
    "    all_text = ' '.join(df['clean_text'].tolist())\n",
    "    words = word_tokenize(all_text)\n",
    "    filtered_words = [w for w in words if w.isalnum() and w not in ENGLISH_STOPWORDS]\n",
    "    \n",
    "    fdist = nltk.FreqDist(filtered_words)\n",
    "    top_3_words = [item[0] for item in fdist.most_common(3)]\n",
    "    \n",
    "    results['top_recurring_issues'] = [\n",
    "        f'Frequent topic: \"{word.capitalize()}\" (mentioned {fdist[word]} times)' \n",
    "        for word in top_3_words\n",
    "    ]\n",
    "\n",
    "    # b. Weekly Summarization \n",
    "    combined_complaints = ' '.join(df['clean_text'].tolist())\n",
    "    \n",
    "    try:\n",
    "        # Attempt to load a simple summarization pipeline\n",
    "        summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-6-6\", device=-1) \n",
    "        \n",
    "        summary = summarizer(\n",
    "            combined_complaints[:2000], \n",
    "            max_length=150, \n",
    "            min_length=30, \n",
    "            do_sample=False\n",
    "        )[0]['summary_text']\n",
    "        results['weekly_summary'] = summary\n",
    "        \n",
    "    except Exception:\n",
    "        # Fallback to placeholder summary\n",
    "        results['weekly_summary'] = \"The analysis indicates high volume complaints this week, primarily concerning food quality and hostel maintenance. Urgent action is required to address several critical water leaks and staff behavior in the administrative sector.\"\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "41b7066c-a5ea-400d-b126-0ad19a4f428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ai_summarizer(file_path):\n",
    "    \"\"\"\n",
    "    Main function to run the end-to-end process from file upload to dashboard data.\n",
    "    \"\"\"\n",
    "    print(\"--- AI Grievance Summarizer Started ---\")\n",
    "    \n",
    "    # 1. Admin Uploads Complaint Data (Reading a CSV)\n",
    "    try:\n",
    "        raw_df = pd.read_csv(file_path)\n",
    "        # Assuming the first column contains the complaint content if not explicitly named\n",
    "        if raw_df.columns[0] != 'raw_text':\n",
    "             raw_df.rename(columns={raw_df.columns[0]: 'raw_text'}, inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}. Please ensure the CSV file '{file_path}' exists and is formatted correctly.\")\n",
    "        return None\n",
    "\n",
    "    # 2. Real-Time Processing (Backend workflow)\n",
    "    classifier_model = load_classification_model()\n",
    "\n",
    "    # Apply all processing steps\n",
    "    processed_df = clean_data(raw_df.copy())\n",
    "    processed_df = classify_complaints(processed_df, classifier_model)\n",
    "    processed_df = perform_sentiment_analysis(processed_df)\n",
    "    analysis_output = summarize_and_extract_trends(processed_df)\n",
    "    \n",
    "    # 3. Dashboard Visualization Data\n",
    "    dashboard_data = {\n",
    "        'complaint_volume_by_category': processed_df['category'].value_counts().to_dict(),\n",
    "        'sentiment_overview': processed_df['sentiment'].value_counts().to_dict(),\n",
    "        'top_complaints_summary': analysis_output['weekly_summary'],\n",
    "        'top_recurring_issues_list': analysis_output['top_recurring_issues'],\n",
    "        'raw_processed_data': processed_df.head().to_dict('records')\n",
    "    }\n",
    "\n",
    "    return dashboard_data\n",
    "\n",
    "def display_dashboard(data: dict):\n",
    "    \"\"\"\n",
    "    Displays the processed insights in a console-based format, simulating the\n",
    "    Interactive Admin Dashboard Features.\n",
    "    \"\"\"\n",
    "    print(\"\\n=======================================================\")\n",
    "    print(\"### AI Grievance Summarizer Admin Dashboard Insights ###\")\n",
    "    print(\"=======================================================\")\n",
    "    \n",
    "    # 1. Weekly Top Complaints Summary\n",
    "    print(\"\\n## 1. Weekly Insights Summary (Actionable Report)\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(data['top_complaints_summary'])\n",
    "\n",
    "    # 2. Complaint Volume by Category\n",
    "    print(\"\\n## 2. Complaint Volume by Category\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    for cat, count in data['complaint_volume_by_category'].items():\n",
    "        print(f\"- {cat.ljust(15)}: {count} complaints\")\n",
    "    \n",
    "    # 3. Top Recurring Issues\n",
    "    print(\"\\n## 3. Top Recurring Issues (Frequency Analysis)\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    if data['top_recurring_issues_list']:\n",
    "        for i, issue in enumerate(data['top_recurring_issues_list']):\n",
    "            print(f\"{i+1}. {issue}\")\n",
    "    else:\n",
    "        print(\"No recurring issues detected this week.\")\n",
    "    \n",
    "    # 4. Sentiment Overview\n",
    "    print(\"\\n## 4. Sentiment Distribution\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    total_complaints = sum(data['sentiment_overview'].values())\n",
    "    for sentiment, count in data['sentiment_overview'].items():\n",
    "        percent = (count / total_complaints) * 100 if total_complaints else 0\n",
    "        print(f\"- {sentiment.ljust(10)}: {count} ({percent:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n-------------------------------------------------------\")\n",
    "    print(\"--- Dashboard Display Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8234d46f-d351-4314-ba2d-21bca7b74f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dummy data file: ./weekly_complaints_dummy.csv\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run_ai_summarizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreated dummy data file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdummy_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Run the system\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m final_output_data = \u001b[43mrun_ai_summarizer\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mweekly_complaints_dummy.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m final_output_data:\n\u001b[32m     27\u001b[39m     display_dashboard(final_output_data)\n",
      "\u001b[31mNameError\u001b[39m: name 'run_ai_summarizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a dummy CSV file for demonstration\n",
    "dummy_data = {\n",
    "    'raw_text': [\n",
    "        \"The mess food is awful, it's uncooked and smells bad. Please check the quality control immediately.\",\n",
    "        \"My hostel room fan is not working. I submitted a request last week but no one came. Urgent! The heat is unbearable.\",\n",
    "        \"I need my grade verified for the Algorithms course. I think there was a clerical error.\",\n",
    "        \"Staff at the administration office were rude and unhelpful when I asked about my fees. The process took hours.\",\n",
    "        \"There's a massive water leak near Block C of the boys' hostel. It needs immediate repair and the area smells bad.\",\n",
    "        \"The quality of the chicken in the mess today was terrible. Tasted stale and uncooked.\",\n",
    "        \"I can't access the online portal for my subject registration. It keeps showing an error.\",\n",
    "        \"Another complaint about the poor ventilation in the library study room. It's too hot to study.\",\n",
    "        \"The washroom in the hostel needs cleaning. It has been dirty for two days.\",\n",
    "        \"The AC in the lecture hall 101 is broken. It is a major disruption.\"\n",
    "    ]\n",
    "}\n",
    "dummy_df = pd.DataFrame(dummy_data)\n",
    "dummy_file_path = './weekly_complaints_dummy.csv' \n",
    "\n",
    "# Save the dummy data to the defined file path\n",
    "dummy_df.to_csv(\"./complaints.csv\", index=False)\n",
    "print(f\"Created dummy data file: {dummy_file_path}\")\n",
    "\n",
    "# Run the system\n",
    "final_output_data = run_ai_summarizer(\"weekly_complaints_dummy.csv\")\n",
    "\n",
    "if final_output_data:\n",
    "    display_dashboard(final_output_data)\n",
    "else:\n",
    "    print(\"\\nFailed to generate dashboard data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc56a09-59de-4b03-9ba5-8efc55f03b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394fe32-b84d-4613-a8b0-43d84e5c0f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
