{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "98540eba-c51f-4108-a4f5-e56e927026d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1677316086.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpython update pip\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9dd918f9-368e-4e74-87f7-3e1f58eb98dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'nltk.downloader' has no attribute 'DownloadError'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtokenizers/punkt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     nltk.data.find(\u001b[33m'\u001b[39m\u001b[33mcorpora/stopwords\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt\u001b[0m\n\n  Searched in:\n    - '/Users/avi19/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m     nltk.data.find(\u001b[33m'\u001b[39m\u001b[33mtokenizers/punkt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m     nltk.data.find(\u001b[33m'\u001b[39m\u001b[33mcorpora/stopwords\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDownloadError\u001b[49m:\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDownloading NLTK resources (punkt, stopwords)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m     nltk.download(\u001b[33m'\u001b[39m\u001b[33mpunkt\u001b[39m\u001b[33m'\u001b[39m, quiet=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'nltk.downloader' has no attribute 'DownloadError'"
     ]
    }
   ],
   "source": [
    "# Core Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text Preprocessing (NLTK)\n",
    "import nltk\n",
    "# Ensure NLTK data is available\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    print(\"Downloading NLTK resources (punkt, stopwords)...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# NLP Models (Hugging Face Transformers - used in subsequent cells)\n",
    "from transformers import pipeline\n",
    "\n",
    "# Define Global Constants\n",
    "CATEGORIES = ['Hostel', 'Mess', 'Academics', 'Administration']\n",
    "ENGLISH_STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "print(\"Setup Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20807a36-8fef-4e0b-9f8c-8f7901ef7881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies cleaning and normalization steps: removes duplicates, anonymizes, \n",
    "    and normalizes text.\n",
    "    \"\"\"\n",
    "    print(\"Step 1: Data Cleaning...\")\n",
    "    # a. Remove duplicate complaints\n",
    "    df.drop_duplicates(subset=['raw_text'], inplace=True)\n",
    "    \n",
    "    # b. Anonymize (Placeholder: simple lowercase/string conversion)\n",
    "    def anonymize_text(text):\n",
    "        if pd.isna(text): return \"\"\n",
    "        return str(text).lower() \n",
    "        \n",
    "    df['clean_text'] = df['raw_text'].apply(anonymize_text)\n",
    "    \n",
    "    # c. Normalize text (remove extra spaces)\n",
    "    df['clean_text'] = df['clean_text'].str.strip()\n",
    "    return df\n",
    "\n",
    "def load_classification_model():\n",
    "    \"\"\"Placeholder for loading a trained scikit-learn classification model.\"\"\"\n",
    "    # In a real project, the model and vectorizer would be loaded here.\n",
    "    return None \n",
    "\n",
    "def classify_complaints(df: pd.DataFrame, classifier=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Classifies complaints into defined categories. Uses simulation if no model is provided.\n",
    "    \"\"\"\n",
    "    print(\"Step 2: Complaint Classification (Simulated)...\")\n",
    "    if classifier:\n",
    "        # Actual model prediction code goes here\n",
    "        pass\n",
    "    \n",
    "    # Simulating classification \n",
    "    weights = [0.35, 0.35, 0.15, 0.15] # Mess and Hostel often have more complaints\n",
    "    df['category'] = np.random.choice(CATEGORIES, size=len(df), p=weights) \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "36d187ac-85f2-4e0a-8e22-e14cfaa39472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_sentiment_analysis(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyzes the emotional tone of complaints. Uses a simulated Hugging Face pipeline \n",
    "    or random choice for portability.\n",
    "    \"\"\"\n",
    "    print(\"Step 3: Sentiment Analysis (Simulated)...\")\n",
    "    \n",
    "    try:\n",
    "        # Attempt to load a simple sentiment pipeline\n",
    "        sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", device=-1)\n",
    "        \n",
    "        def get_sentiment(text):\n",
    "            if not text: return 'Neutral'\n",
    "            try:\n",
    "                result = sentiment_pipeline(text[:250])[0]\n",
    "                return result['label'].capitalize()\n",
    "            except Exception:\n",
    "                return 'Neutral'\n",
    "\n",
    "        df['sentiment'] = df['clean_text'].apply(get_sentiment)\n",
    "        \n",
    "    except Exception:\n",
    "        # Fallback to simulation \n",
    "        df['sentiment'] = np.random.choice(['Negative', 'Neutral', 'Positive'], size=len(df), p=[0.7, 0.2, 0.1])\n",
    "\n",
    "    # Placeholder for 'Urgency' assessment\n",
    "    df['urgency'] = np.random.choice(['High', 'Medium', 'Low'], size=len(df), p=[0.2, 0.5, 0.3])\n",
    "    return df\n",
    "\n",
    "    \n",
    "def summarize_and_extract_trends(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Identifies top recurring issues via frequency and generates a weekly summary \n",
    "    using a transformer model (simulated).\n",
    "    \"\"\"\n",
    "    print(\"Step 4: Summarization and Trend Extraction...\")\n",
    "    results = {}\n",
    "    \n",
    "    # a. Trend Extraction (Top 3 Recurring Issues)\n",
    "    all_text = ' '.join(df['clean_text'].tolist())\n",
    "    words = word_tokenize(all_text)\n",
    "    filtered_words = [w for w in words if w.isalnum() and w not in ENGLISH_STOPWORDS]\n",
    "    \n",
    "    fdist = nltk.FreqDist(filtered_words)\n",
    "    top_3_words = [item[0] for item in fdist.most_common(3)]\n",
    "    \n",
    "    results['top_recurring_issues'] = [\n",
    "        f'Frequent topic: \"{word.capitalize()}\" (mentioned {fdist[word]} times)' \n",
    "        for word in top_3_words\n",
    "    ]\n",
    "\n",
    "    # b. Weekly Summarization \n",
    "    combined_complaints = ' '.join(df['clean_text'].tolist())\n",
    "    \n",
    "    try:\n",
    "        # Attempt to load a simple summarization pipeline\n",
    "        summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-6-6\", device=-1) \n",
    "        \n",
    "        summary = summarizer(\n",
    "            combined_complaints[:2000], \n",
    "            max_length=150, \n",
    "            min_length=30, \n",
    "            do_sample=False\n",
    "        )[0]['summary_text']\n",
    "        results['weekly_summary'] = summary\n",
    "        \n",
    "    except Exception:\n",
    "        # Fallback to placeholder summary\n",
    "        results['weekly_summary'] = \"The analysis indicates high volume complaints this week, primarily concerning food quality and hostel maintenance. Urgent action is required to address several critical water leaks and staff behavior in the administrative sector.\"\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "41b7066c-a5ea-400d-b126-0ad19a4f428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ai_summarizer(file_path):\n",
    "    \"\"\"\n",
    "    Main function to run the end-to-end process from file upload to dashboard data.\n",
    "    \"\"\"\n",
    "    print(\"--- AI Grievance Summarizer Started ---\")\n",
    "    \n",
    "    # 1. Admin Uploads Complaint Data (Reading a CSV)\n",
    "    try:\n",
    "        raw_df = pd.read_csv(file_path)\n",
    "        # Assuming the first column contains the complaint content if not explicitly named\n",
    "        if raw_df.columns[0] != 'raw_text':\n",
    "             raw_df.rename(columns={raw_df.columns[0]: 'raw_text'}, inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}. Please ensure the CSV file '{file_path}' exists and is formatted correctly.\")\n",
    "        return None\n",
    "\n",
    "    # 2. Real-Time Processing (Backend workflow)\n",
    "    classifier_model = load_classification_model()\n",
    "\n",
    "    # Apply all processing steps\n",
    "    processed_df = clean_data(raw_df.copy())\n",
    "    processed_df = classify_complaints(processed_df, classifier_model)\n",
    "    processed_df = perform_sentiment_analysis(processed_df)\n",
    "    analysis_output = summarize_and_extract_trends(processed_df)\n",
    "    \n",
    "    # 3. Dashboard Visualization Data\n",
    "    dashboard_data = {\n",
    "        'complaint_volume_by_category': processed_df['category'].value_counts().to_dict(),\n",
    "        'sentiment_overview': processed_df['sentiment'].value_counts().to_dict(),\n",
    "        'top_complaints_summary': analysis_output['weekly_summary'],\n",
    "        'top_recurring_issues_list': analysis_output['top_recurring_issues'],\n",
    "        'raw_processed_data': processed_df.head().to_dict('records')\n",
    "    }\n",
    "\n",
    "    return dashboard_data\n",
    "\n",
    "def display_dashboard(data: dict):\n",
    "    \"\"\"\n",
    "    Displays the processed insights in a console-based format, simulating the\n",
    "    Interactive Admin Dashboard Features.\n",
    "    \"\"\"\n",
    "    print(\"\\n=======================================================\")\n",
    "    print(\"### AI Grievance Summarizer Admin Dashboard Insights ###\")\n",
    "    print(\"=======================================================\")\n",
    "    \n",
    "    # 1. Weekly Top Complaints Summary\n",
    "    print(\"\\n## 1. Weekly Insights Summary (Actionable Report)\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    print(data['top_complaints_summary'])\n",
    "\n",
    "    # 2. Complaint Volume by Category\n",
    "    print(\"\\n## 2. Complaint Volume by Category\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    for cat, count in data['complaint_volume_by_category'].items():\n",
    "        print(f\"- {cat.ljust(15)}: {count} complaints\")\n",
    "    \n",
    "    # 3. Top Recurring Issues\n",
    "    print(\"\\n## 3. Top Recurring Issues (Frequency Analysis)\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    if data['top_recurring_issues_list']:\n",
    "        for i, issue in enumerate(data['top_recurring_issues_list']):\n",
    "            print(f\"{i+1}. {issue}\")\n",
    "    else:\n",
    "        print(\"No recurring issues detected this week.\")\n",
    "    \n",
    "    # 4. Sentiment Overview\n",
    "    print(\"\\n## 4. Sentiment Distribution\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "    total_complaints = sum(data['sentiment_overview'].values())\n",
    "    for sentiment, count in data['sentiment_overview'].items():\n",
    "        percent = (count / total_complaints) * 100 if total_complaints else 0\n",
    "        print(f\"- {sentiment.ljust(10)}: {count} ({percent:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n-------------------------------------------------------\")\n",
    "    print(\"--- Dashboard Display Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8234d46f-d351-4314-ba2d-21bca7b74f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dummy data file: ./weekly_complaints_dummy.csv\n",
      "--- AI Grievance Summarizer Started ---\n",
      "Step 1: Data Cleaning...\n",
      "Step 2: Complaint Classification (Simulated)...\n",
      "Step 3: Sentiment Analysis (Simulated)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Summarization and Trend Extraction...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreated dummy data file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdummy_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Run the system\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m final_output_data = \u001b[43mrun_ai_summarizer\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweekly_complaints_dummy.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m final_output_data:\n\u001b[32m     27\u001b[39m     display_dashboard(final_output_data)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mrun_ai_summarizer\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m     22\u001b[39m processed_df = classify_complaints(processed_df, classifier_model)\n\u001b[32m     23\u001b[39m processed_df = perform_sentiment_analysis(processed_df)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m analysis_output = \u001b[43msummarize_and_extract_trends\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# 3. Dashboard Visualization Data\u001b[39;00m\n\u001b[32m     27\u001b[39m dashboard_data = {\n\u001b[32m     28\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcomplaint_volume_by_category\u001b[39m\u001b[33m'\u001b[39m: processed_df[\u001b[33m'\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m'\u001b[39m].value_counts().to_dict(),\n\u001b[32m     29\u001b[39m     \u001b[33m'\u001b[39m\u001b[33msentiment_overview\u001b[39m\u001b[33m'\u001b[39m: processed_df[\u001b[33m'\u001b[39m\u001b[33msentiment\u001b[39m\u001b[33m'\u001b[39m].value_counts().to_dict(),\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mraw_processed_data\u001b[39m\u001b[33m'\u001b[39m: processed_df.head().to_dict(\u001b[33m'\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     33\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36msummarize_and_extract_trends\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# a. Trend Extraction (Top 3 Recurring Issues)\u001b[39;00m\n\u001b[32m     40\u001b[39m all_text = \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(df[\u001b[33m'\u001b[39m\u001b[33mclean_text\u001b[39m\u001b[33m'\u001b[39m].tolist())\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m words = \u001b[43mword_tokenize\u001b[49m(all_text)\n\u001b[32m     42\u001b[39m filtered_words = [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m w.isalnum() \u001b[38;5;129;01mand\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ENGLISH_STOPWORDS]\n\u001b[32m     44\u001b[39m fdist = nltk.FreqDist(filtered_words)\n",
      "\u001b[31mNameError\u001b[39m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a dummy CSV file for demonstration\n",
    "dummy_data = {\n",
    "    'raw_text': [\n",
    "        \"The mess food is awful, it's uncooked and smells bad. Please check the quality control immediately.\",\n",
    "        \"My hostel room fan is not working. I submitted a request last week but no one came. Urgent! The heat is unbearable.\",\n",
    "        \"I need my grade verified for the Algorithms course. I think there was a clerical error.\",\n",
    "        \"Staff at the administration office were rude and unhelpful when I asked about my fees. The process took hours.\",\n",
    "        \"There's a massive water leak near Block C of the boys' hostel. It needs immediate repair and the area smells bad.\",\n",
    "        \"The quality of the chicken in the mess today was terrible. Tasted stale and uncooked.\",\n",
    "        \"I can't access the online portal for my subject registration. It keeps showing an error.\",\n",
    "        \"Another complaint about the poor ventilation in the library study room. It's too hot to study.\",\n",
    "        \"The washroom in the hostel needs cleaning. It has been dirty for two days.\",\n",
    "        \"The AC in the lecture hall 101 is broken. It is a major disruption.\"\n",
    "    ]\n",
    "}\n",
    "dummy_df = pd.DataFrame(dummy_data)\n",
    "dummy_file_path = './weekly_complaints_dummy.csv' \n",
    "\n",
    "# Save the dummy data to the defined file path\n",
    "dummy_df.to_csv(\"./complaints.csv\", index=False)\n",
    "print(f\"Created dummy data file: {dummy_file_path}\")\n",
    "\n",
    "# Run the system\n",
    "final_output_data = run_ai_summarizer(\"weekly_complaints_dummy.csv\")\n",
    "\n",
    "if final_output_data:\n",
    "    display_dashboard(final_output_data)\n",
    "else:\n",
    "    print(\"\\nFailed to generate dashboard data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc56a09-59de-4b03-9ba5-8efc55f03b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
